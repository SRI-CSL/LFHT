\section{Algorithm}
\label{sec:algo}

We develop a hash table that maps integral keys to integral
values. The implementation supports insertion of new $(key,value)$
pairs, updates of the value assigned to an existing key, removal of a
key, and search. Updates and insertions are implemented by a single
{\em add\/} operation. These operations can be performed concurrently
by multiple threads. In the typical case, none of these operations
require obtaining a lock.

The underlying hash table algorithm we use is linear probing. The
hash-table content is stored in a shared array. Each entry in that
array is a pair that consists of 64-bit integers or pointers. Updates
to this array are performed atomically using compare-and-swap (CAS)
operations. Our algorithm attempts to keep the array less than 70\%
full (this can be configured at compile time). When this threshold is
exceeded, we allocate a fresh array of double the current capacity and
migrate entries from the current array into this fresh array.\footnote{Mention tombstones}

\todo{A little sentence here to smooth the transition}.

\subsection{Overview}

Our hash table uses 64-bit keys and values. Table entries must be
aligned on addresses that are multiples of four. These constraints are
imposed by the CAS operations on Intel hardware. We also use the low-order
bit of the keys as a mark.

The hash table consists of a header and a linked list of
dynamically-allocated arrays.  The first element in this list is the
most recent copy of the table. The next element is an older copy and
so forth.  In most cases, threads operate on the most current table
(i.e., the first one in the list). When this table becomes full, we
allocate a fresh array of double its size and add it at the front of
the list.  At this point, all threads that operate on the table are
required to migrate entries from the old table to the new current
table, the head of the list. A key invariant that our algorithm
maintains is that migration is always from the second table in the
list to the first table. In other words, the current table cannot
become full before the previous table has been completely migrated.
We say then that this table is {\em assimilated\/}.

During the migration process, a thread that operates on the table must
first copy a fixed number $M$ of key/values pairs from the old table
to the new table. The number $M$ is chosen so that a new table cannot
become full before the old table has been assimilated.

To avoid running out of memory, it is desirable to free old copies of
the table that are no longer needed. But, because of thread
scheduling, some threads may keep pointers to old versions of the
tables (i.e., elements deep in the linked list). We can only free old
copies when we are sure that no threads have pointers to them. We use
reference counting for this purpose. Maintaining correct reference
counts introduces race conditions that we could not eliminate with
CAS.  We then introduced a locking mechanism, which is used only in
critical operations that are part of the migration process. Typical
accesses to the table do not require locking.


\subsection{Data Structures}

\begin{figure}
\begin{center}
\begin{clisting}
typedef struct lfht_entry_s {
  volatile uint64_t  key;
  volatile uint64_t  val;
} lfht_entry_t;

typedef struct lfht_hdr_s lfht_hdr_t;

struct lfht_hdr_s {
  atomic_uint_least32_t reference_count;
  atomic_bool assimilated;
  uint64_t sz;
  uint32_t max;
  uint32_t threshold;
  atomic_uint_least32_t count;
  lfht_hdr_t *next;
  lfht_entry_t *table;
};

typedef struct lfht_s {
  uintptr_t hdr:62, state:2;
  pthread_mutex_t lock;
} lfht_t;
\end{clisting}
\end{center}
\caption{Data Structures}
\label{fig:datastructures}
\end{figure}

Figure~\ref{fig:datastructures} shows the main data structures. A hash
table consists of a {\em descriptor\/} of type \texttt{lfht\_t} that
stores a state, a pointer to the list of table headers, and a lock.
The pointer to the list and the state are stored as a single 64 bit integer
to allow us to atomically update them.
Each
element of the list is a structure of type \texttt{lfht\_hdr\_t}. It
stores an array of entries, each of type \texttt{lfht\_entry\_t}, counters and sizes, and a pointer to the
next list element.

The operations to deconstruct the bit fields of \texttt{lfht\_t} object are the following three
routines.
\begin{center}
\begin{clisting}
static inline lfht_hdr_t *lfht_table_hdr(const lfht_t* ht){
  return (lfht_hdr_t *)(uintptr_t)(ht->hdr << 2);
}

static inline lfht_state_t lfht_state(const lfht_t* ht){
  return ht->state;
}

static inline void lfht_set(lfht_t* ht, lfht_hdr_t * hdr, lfht_state_t state){
  ht->state = state;
  ht->hdr = (uintptr_t)hdr >> 2;
}
\end{clisting}
\end{center}


Even though we can atomically update the \texttt{state} and
\texttt{hdr} fields of a table descriptor\footnote{Use this
  terminology uniformly?}  we protect them by a lock. There are three
places in our algorithm where we rely on the fact that if we obtain
the lock, then the \texttt{state} and \texttt{hdr} fields will not
change.  These are when we first initialize a handle, when we grow the
table, and when we see if we can reclaim an old assimilated table.

Each of these three places have resisted all our efforts to eliminate the race conditions
with non-locking means. We have had no success.


A table can be in one of four states:

\begin{center}
\begin{clisting}
typedef enum lfht_state { INITIAL, EXPANDING, EXPANDED, FINAL } lfht_state_t;
\end{clisting}
\end{center}

1. The current table is the only table. 2. There is more than one table, the current table is new, and its
predecessor still contains key-value pairs that have not yet been
migrated to the new table. 3. There is more than one table, the current table is new, and its
predecessor table has been assimilated, all key-value pairs have been
migrated to the new table. 4. The table has been reclaimed.

So as a first approximation, our hash table consists of a state and a
pointer to the list header, the front of the linked list of tables.
The question then, is "how can we release old tables?" The problem is
that, without additional information, there could always be slow
threads that wake up thinking that the old table is the current active
table.

We solve this problem by using reference counting.
All hash table operations must be made using a per-thread {\em handle\/}. The handle is  a
structure of type \texttt{lfht\_handle\_t}, and stores
the identifier of the thread that owns it, a pointer to the table, a pointer to the handle's notion
of the current table header, and the oldest table header that the thread has an interest in.

\begin{center}
\begin{clisting}
typedef struct lfht_handle_s {
  pthread_t owner;
  lfht_t *table;
  lfht_hdr_t *table_hdr;
  lfht_hdr_t *last;  
} lfht_handle_t;
\end{clisting}
\end{center}

Of particular interest is the thread's current idea of which
is the active table, and what is the oldest table the thread has
an active interest in.


When a thread creates a handle to a table, it increments the reference
count of the current table, and if the table is expanding, also the
next table in the linked list, which is the handle's \texttt{last} field.  If
the table is not expanding, \texttt{last} is just the current table header.

Note that a handle may not be upto date. The list
header stored in the \texttt{table} field may in fact point to a newer table header,
and not the handle's \texttt{table\_hdr}
field. However on creation we must ensure that it is upto date, and
this is one of the critical sections that we must protect with a lock.
An crucial invariant that we maintain is that a thread has incremented
every table header from the it's \texttt{table\_hdr} to it's \texttt{last}
field.

Periodically a thread will ensure that its handle is upto date. During this process
it may move it's last field, and decrement reference counts. Similarly
at periodic intervals the last table in the linked list will be checked to see
if it be reclaimed (i.e. has zero) reference count.


\subsection{Algorithms}


We describe the process of initializing a handle, updating a handle, and then using that handle
to add an entry to the table. In the process will will have to describe both the growing of the table,
and the migration of entries from the previous table to the
current.\footnote{Do we also want to describe the reclamation process?}
The actual creation of the table is routine,
and can be gleaned from the source code\cite{lfht2017}.


The process of initializing a \texttt{lfht\_handle\_t} object is relatively simple.
We set the \texttt{owner} and  \texttt{table} fields to the appropriate objects.
We then block until we have obtained the table's \texttt{lock}. Once we have the lock
we can retrieve the current table header, increment its reference count, and
assign it to the handle's \texttt{table\_hdr} field. If the previous table header
is not assimilated, we presume that the thread maybe required to migrate entries,
and so we increment its reference count as well. Recording the fact that
the \texttt{last} field points to the {\em last}  \texttt{table\_hdr} in the list
whose reference count we have incremented.
We must protect this operation by obtaining the mutex to ensure that the \texttt{actual}
is in fact pointing to the current table header.


\begin{center}
\begin{clisting}
void init_handle(lfht_handle_t *h, lfht_t* table){
  lfht_hdr_t *actual, *next;
  h->owner = pthread_self();
  h->table = table;
  /* the lock is to prevent the table->table_hdr from changing before we can increment the counter */
  pthread_mutex_lock(&table->lock);
  actual = lfht_table_hdr(table);
  /*
   * if the lock was not held, actual could by this point be out of date, and so we would actually
   * miss incrementing the current table's reference count.
   */
  next = actual->next;
  atomic_fetch_add(&actual->reference_count, 1);
  /* 
   * if there is an old table, and it is not assimilated, then we will
   * probably need to help migrate it, so we increment its reference count.
   *
   */
  if(next != NULL && !table_is_assimilated(next)){
    atomic_fetch_add(&next->reference_count, 1);
    h->last = next;
  } else {
    h->last = actual;
  }
  h->table_hdr = actual;
  pthread_mutex_unlock(&table->lock);
}
\end{clisting}
\end{center}

While we take care to ensure that upon initialization a thread's handle is upto date,
we cannot maintain such an invariant. However the handle is designed so that it is particularly
easy to determine if it is upto date or not, using the  \texttt{upto\_date} routine.
A thread's handle is no longer upto date in the situation when the table has been grown by another thread.
Once a thread determines that it's handle is no longer upto date, it must rectify the problem.
Note that because we have no control over thread scheduling, we cannot assume that
the table has only been grown once. In fact it is reasonable to assume that the table
has in fact been grown an arbitrary finite number of times. As a result updating
a handle requires not only updating the table header field, but also making sure
that our reference counts are accurate. This involves incrementing the reference counts
of new table headers, and decrementing the counts of our, now stale, older table headers.
It is in this process that we rely on the fact that the handle's \texttt{last} field
points to the oldest table whose reference count has been incremented by the handle's owning thread.
Also note that since a handle is local to a thread, there are by definition no races here.

\begin{center}
\begin{clisting}
static bool upto_date(lfht_handle_t *h){
  return lfht_table_hdr(h->table) == h->table_hdr;
}

static void update_handle(lfht_handle_t *h){
  lfht_hdr_t *p;
  bool assimilated;
  lfht_t *table = h->table;
  lfht_hdr_t *actual = lfht_table_hdr(table);
  lfht_hdr_t *current  = h->table_hdr;
  lfht_hdr_t *last = h->last;
  int prior_count = 2;
  if(actual == current){
    return;
  }
  h->table_hdr = actual;
  /* increment to the left */
  p = actual;
  while(true){
    atomic_fetch_add(&p->reference_count, 1);
    p = p->next;
    if(p == current){ break; }
  }
  if(current == last){  return; }
  /* decrement to the right till last */
  p = current->next;
  while(true){
    assimilated = table_is_assimilated(p);
    assert(assimilated);
    prior_count = atomic_fetch_sub(&p->reference_count, 1);
    if(p == last){ break; }
    p = p->next;
  }
  h->last = current;
  if(prior_count == 1){
    /* good time to do a free table check? */
    free_table_check(h->table, "update_handle", true);
  }
}
\end{clisting}
\end{center}

Adding a key value pair to a table using the \texttt{lfht\_add} is a good illustration of
how we use the handle. We first compute the hash of the key. We then attempt to participate in any
migrating that might be required. We will describe the migration process in detail shortly, and
simply make two remarks in passing. It is important that we begin the migration from the hash of the
key we are interesting in, because we can then be confident that its current value, if any, is in
the current table. Secondly the migration process endevors to ensure that the thread handle is upto date.
Once we have performed our migration duties, we then proceed to add the key value pair to the
table \texttt{\_lfht\_add}, if we succeed, and the handle is upto date still, then we increment
the table header's count, and check to see if the table needs to grow.
If we succeeded but our handle is no longer upto date, then we have added to the old table,
and we need to try again with the new table. If we did not succeed, then either that table
was full, or else the key has already been assimilated (and we are not  upto date). In either case
we need to update our handle and try again. Note that if we failed because the table is full, then
we are relying on another thread to grow the table. This is sound since each of those threads that
successfully added to the table beyond its threshold will try their luck at growing the table. One will
succeed. When we finally successfully added to the table, and are upto date, we increment the
table header's entry \texttt{count}, and if the new value is no longer smaller than the threshold
we attempt to grow the table.




\begin{center}
\begin{clisting}
bool lfht_add(lfht_handle_t *h, uint64_t key, uint64_t val){
  bool retval, current;
  lfht_hdr_t *table_hdr;
  uint_least32_t count;
  uint32_t hash = jenkins_hash_ptr((void *)key);
  while(true){
    _migrate_table(h, key, hash);
    retval = _lfht_add(h, key, hash, val);
    current = upto_date(h);
    if( retval && current ){ break; }
    if( ! current ){ update_handle(h); }
  }
  table_hdr = h->table_hdr;
  count = atomic_load(&table_hdr->count);
  if (count >= table_hdr->threshold){
    _grow_table(h->table, table_hdr);
  }
  return retval;
}

\end{clisting}
\end{center}


A thread calls to \texttt{\_grow\_table} when the thread's current table header has reached
the threshold. It is quite likely that there will be contention for this task, so 
we pass in the thread's current table header so as to be able to detect if the table
has already grown. The actual task of growing the table is relatively simple, we allocate
a new table header that is twice the size\footnote{Since our main application is a memory
allocator, we use \texttt{mmap} to allocate table header memory.}
and set it as the current header, and the state of the table to be \texttt{EXPANDING}.
We do this under the protection of the lock, not because there are any race conditions
involved in the process (we could easily do this atomically with a 64 bit CAS), but because
other threads may be in delicate operations, such a initializing a new handle to the table,
or trndling down the table header list looking for tables to reclaim.


%grow_table
\begin{center}
\begin{clisting}
bool _grow_table(lfht_t *ht,  lfht_hdr_t *hdr){
  lfht_hdr_t *ohdr, *nhdr;
  bool retval = false;
  uint32_t omax, nmax;
  pthread_mutex_lock(&ht->lock);
  ohdr = lfht_table_hdr(ht);
  /* check to see if someone beat us to it */
  if(hdr == ohdr){
    omax = ohdr->max;
    if (omax < MAX_TABLE_SIZE) {
      nmax = 2 * ohdr->max;
      nhdr  = alloc_lfht_hdr(nmax);
      if (nhdr != NULL){
	nhdr->next = ohdr;
	lfht_set(ht, nhdr, EXPANDING);
	retval = true;
      }
    }
  }
  pthread_mutex_unlock(&ht->lock);
  return retval;
}

\end{clisting}
\end{center}

The routine \texttt{\_migrate\_table} attempts to move \texttt{MIGRATIONS\_PER\_ACCESS} key-value
pairs from the old table header to the new table header. The routine is careful to be sensitive to
the possibility that the table state may change.
The workhorse of this process
is the subroutine  \texttt{assimilate} which returns the number of key-value pairs actually moved.
If this number is less than the requested amount, we can safely assume that the table has been
fully migrated. In this case we atomically update the \texttt{assimilated} field of the old table header,
and then, while holding the lock, update the state of the stable from \texttt{EXPANDING} to
\texttt{EXPANDED}.\footnote{Race condition: not requiring that the thread updating the
  table state holds the lock, introduces a nasty race condition, where the thread being a bit slow
  actually toggles the state from \texttt{EXPANDING} to \texttt{EXPANDED} for a migration that
  has just commenced, not the previous one that just finished.}
As mentioned earlier, when moving key-value pairs from the old table header to the new table header, we
commence the emigration at the hash of the key of the API operation that triggered the call to \texttt{\_migrate\_table}.




%migrate_table
\begin{center}
\begin{clisting}
static int32_t _migrate_table(lfht_handle_t *h, uint64_t key, uint32_t hash){
  int table_state;
  lfht_t *ht;
  lfht_hdr_t *hdr, *ohdr;
  uint32_t moved;
  bool finished, assimilated;
  int32_t migrated = -1;
  
 restart:

  update_handle(h);
  ht = h->table;
  table_state = lfht_state(ht);
  if (table_state == EXPANDING){
    /* gotta try and pitch in and do some migrating */
    hdr = lfht_table_hdr(ht);
    ohdr = hdr->next;
    if (!upto_date(h) ){ goto restart; }
    if(hdr == h->last){ return migrated; }
    moved = assimilate(h, ohdr, key, hash,  MIGRATIONS_PER_ACCESS);
    finished = moved < MIGRATIONS_PER_ACCESS;
    assimilated = false;
    migrated = moved;
    if (finished  &&  atomic_compare_exchange_strong(&ohdr->assimilated, &assimilated, true)){
      pthread_mutex_lock(&ht->lock);
      /* we hold the lock so ht->table_hdr cannot change while we are in here. */
      if (lfht_state(ht) == EXPANDING  && lfht_table_hdr(ht) == hdr){
	lfht_set(ht, hdr, EXPANDED);
        free_table_check(h->table, "_migrate_table", false);
      }
      pthread_mutex_unlock(&ht->lock);
    }
  }
  return migrated;
}
\end{clisting}
\end{center}

The \texttt{assimilate} routine starts out at the slot corresponding to the \texttt{hash} and attempts to
move the \texttt{key} and its associated value to the new table if it is present, and is not tombstoned. Along the way it also tries to
move \texttt{count - 1} many other non-assimilated non-tombstoned key-value pairs. The actual move is done by the routine \texttt{\_lfht\_move},
which we will describe shortly.  For each key that we successfully move, we subsequently mark as assimilated, and increment the counter.
As a result many threads may try and
move the same key-value pair, but we prefer this to the flawed situation where we first mark the key as assimilated, then
move.  The problem with marking the key as assimilated, moving, then updating the counters, is that the last thread to do this (before the
table is assimilated) can go to sleep before the increment, and never get to increment the counter before the new table fills up.


%assimilate
\begin{center}
\begin{clisting}
static uint32_t assimilate(lfht_handle_t *h, lfht_hdr_t *from_hdr, uint64_t key, uint32_t hash,  uint32_t count){
  uint32_t retval, mask, j, i;
  lfht_entry_t current, *table;
  uint64_t akey;
  /* success indicates that the key is either not in the table or has been assimilated */
  bool success = false  
  retval = 0;
  mask = from_hdr->max - 1;
  table = from_hdr->table;
  if (table_is_assimilated(from_hdr)) {
    return retval;
  }
  j = hash & mask;
  i = j;
  while (true) {
    current = table[i];
    if (current.key == 0 || key_equal(current.key, key)) {
      success = true;
    }
    /*
     * move the current entry to the new table if it has the key we want,
     * or if its key is non zero and not assimilated and we haven't moved our quota yet.
     */
    if (current.key == key ||
	(retval < count && current.key != 0 && !key_is_assimilated(current.key))) {
      /*
       * ***WARNING****
       *
       *  we cannot CAS the key, then move the key val pair, then increment the counters
       *  because we could be the last one, and go to sleep before we increment,
       *  and never wake before the new table fills.
       *
       *  an actual bug that was revealed by the logging.
       */
      if (current.val != TOMBSTONE){
	_lfht_move(h, current.key, jenkins_hash_ptr((void *)current.key), current.val);
      }
      akey = set_assimilated(current.key);
      if(cas_64((volatile uint64_t *)&(table[i].key), current.key, akey)){
	retval ++;
      }
    }
    if (success && retval >= count){
      break;
    }
    i++;
    i &= mask;
    if ( i == j ){
      break;
    }
  }
  return retval;
}
\end{clisting}
\end{center}


We are left with describing the two operations that deal with the linear probing aspects
of the underlying table:  \texttt{\_lfht\_move} and \texttt{\_lfht\_add}. The move operation
is the workhorse of \texttt{assimilation} and is simpler in that it can assume that it either
adds the new key-value pair, or else does nothing, if the key is already in the table, it
can safely assume that some other thread has already performed the task. As a result
the move operation should never fail. \texttt{\_lfht\_add} on the other hand must
handle both the replace case, and the situation where the key has been marked as assimilated.
This latter case can arise if the table has grown out from under the thread, and other threads
have been busily migrating keys in the meantime.

Both operations take a handle, a key, the hash of the key, and the
desired value. They commence traversing the array at the position
associated with the hash, looking for an empty slot, or a slot that is
already populated by the same key (or an assimilated variant in the
case of \texttt{\_lfht\_add}). If an empty slot is found, then both operations
perform a 128 bit CAS, replacing both the key and value in one atomic action.
If the key is already present, then the \texttt{\_lfht\_move} operation simply
returns; the \texttt{\_lfht\_add} operation performs a 128 bit CAS if the key
is not marked as assimilated, otherwise it fails. It is important that these operations
perform a 128 bit CAS in the dynamically expanding scenario.\footnote{Race Condition: if one simply
  did a 64 bit CAS of the key followed by 64 bit CAS of the value, then key value pairs can get lost.
  This happens when one thread is adding a new key value pair to the table, but is operating on the
  old table. Once it adds the key, another thread that is migrating values could look at the
  key value pair, deduce that it has been tombstoned, and mark it as assimilated. In this way
  the new value can be lost.}



%_lfht_move
\begin{center}
\begin{clisting}
static void _lfht_move(lfht_handle_t *h, uint64_t key, uint32_t hash, uint64_t val){
  uint32_t mask, j, i;
  lfht_hdr_t *hdr;
  lfht_entry_t*  table;
  lfht_entry_t current;
  lfht_entry_t desired;
  lfht_t *ht;

  ht = h->table;
  hdr = lfht_table_hdr(ht);
  table = hdr->table;
  mask = hdr->max - 1;
  j = hash & mask;
  i = j;
  desired.key = key;
  desired.val = val;
  while (true) {
    current = table[i];
    if (current.key == 0){
      /* 
       * have to do a 128 bit cas here, otherwise a slow move thread can let a find thread think
       * this key has been tombstoned
       */
      if (cas_128((volatile u128_t *)&(table[i]), *((u128_t *)&current),  *((u128_t *)&desired))){
        atomic_fetch_add(&hdr->count, 1);
        goto exit;
      } else {
        continue;
      }
    }
    if ( current.key ==  key ){
      goto exit;
    }
    i++;
    i &= mask;
    if (i == j) break;
  }
  /* should never fail */
  assert(false);
 exit:
}
\end{clisting}
\end{center}




%_lfht_add
\begin{center}
\begin{clisting}
static bool _lfht_add(lfht_handle_t *h, uint64_t key, uint32_t hash, uint64_t val){
  uint32_t mask, j, i;
  lfht_hdr_t *hdr;
  lfht_entry_t*  table;
  lfht_entry_t current;
  lfht_entry_t desired;
  bool retval;
  lfht_t *ht;
  ht = h->table;
  retval = false;
  hdr = lfht_table_hdr(ht);
  table = hdr->table;
  mask = hdr->max - 1;
  j = hash & mask;
  i = j;
  desired.key = key;
  desired.val = val;
  while (true) {
    current = table[i];
    if (current.key == 0){
      /* 
       * have to do a 128 bit cas here, otherwise a slow add thread can let a find thread think
       * this key has been tombstoned
       */
      if (cas_128((volatile u128_t *)&(table[i]), *((u128_t *)&current),  *((u128_t *)&desired))){
        atomic_fetch_add(&hdr->count, 1);
	retval = true;
        goto exit;
      } else {
        continue;
      }
    }
    if ( key_equal(current.key, key) ){
      if( key_is_assimilated(current.key) ){
        retval = false;
        goto exit;
      } else {
	if (cas_128((volatile u128_t *)&(table[i]), *((u128_t *)&current),  *((u128_t *)&desired))){
          retval = true;
          goto exit;
        } else {
          continue;
        }
      }
    }
    i++;
    i &= mask;
    if (i == j) break;
  }
 exit:
  return retval;
}
\end{clisting}
\end{center}
